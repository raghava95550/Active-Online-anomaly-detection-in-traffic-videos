{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from operator import itemgetter\n",
    "\n",
    "from scipy.linalg import cholesky, cho_solve, solve\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.special import erf, expit\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.gaussian_process.kernels import RBF, CompoundKernel, ConstantKernel as C\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_objects:\n",
    "    def __init__(self,clip_no,stime,etime,weights_data,label):\n",
    "        self.clip_no = clip_no\n",
    "        self.stime = stime\n",
    "        self.etime = etime\n",
    "        self.weights_data = weights_data\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_objects = []\n",
    "with open(\"E:\\\\Study\\\\Sem Project\\\\Data\\\\hdp_data.csv\",'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        #from csv file all items will be read as strings, so we have to convert to int if neccesary\n",
    "        total_data_objects.append(data_objects(row[0],row[1],row[2],np.asarray(list(map(float,row[4:]))),int(row[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing objects value.\n",
    "print(\"total no of objects\",len(total_data_objects))\n",
    "for i in range(len(total_data_objects)):\n",
    "    print(\"clip no=\",total_data_objects[i].clip_no)\n",
    "    print(\"start time=\",total_data_objects[i].stime)\n",
    "    print(\"end time=\",total_data_objects[i].etime)\n",
    "    print(\"weights =\",total_data_objects[i].weights_data)\n",
    "    print(\"label = \",total_data_objects[i].label,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of weights\n",
    "max_len = len(total_data_objects[0].weights_data)\n",
    "for i in range(len(total_data_objects)):\n",
    "    if max_len < len(total_data_objects[i].weights_data):\n",
    "        max_len = len(total_data_objects[i].weights_data)\n",
    "        \n",
    "for i in range(len(total_data_objects)):\n",
    "    for j in range(max_len - len(total_data_objects[i].weights_data)):\n",
    "        total_data_objects[i].weights_data = np.append(total_data_objects[i].weights_data,np.mean(total_data_objects[i].weights_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing weights\n",
    "for i in range(len(total_data_objects)):\n",
    "    sum = np.sum(total_data_objects[i].weights_data)\n",
    "    for j in range(len(total_data_objects[i].weights_data)):\n",
    "        total_data_objects[i].weights_data[j] = total_data_objects[i].weights_data[j] / sum\n",
    "    total_data_objects[i].weights_data = np.asarray(total_data_objects[i].weights_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDAS = np.array([0.41, 0.4, 0.37, 0.44, 0.39])[:, np.newaxis]\n",
    "COEFS = np.array([-1854.8214151, 3516.89893646, 221.29346712,\n",
    "                  128.12323805, -2010.49422654])[:, np.newaxis]\n",
    "\n",
    "\n",
    "class _BinaryGaussianProcessClassifierLaplace(BaseEstimator):\n",
    "    def __init__(self, kernel=None, optimizer=\"fmin_l_bfgs_b\",\n",
    "                 n_restarts_optimizer=0, max_iter_predict=100,\n",
    "                 warm_start=False, copy_X_train=True, random_state=None):\n",
    "        self.kernel = kernel\n",
    "        self.optimizer = optimizer\n",
    "        self.n_restarts_optimizer = n_restarts_optimizer\n",
    "        self.max_iter_predict = max_iter_predict\n",
    "        self.warm_start = warm_start\n",
    "        self.copy_X_train = copy_X_train\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Gaussian process classification model.\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        if self.kernel is None:  # Use an RBF kernel as default\n",
    "            self.kernel_ = C(1.0, constant_value_bounds=\"fixed\") \\\n",
    "                * RBF(1.0, length_scale_bounds=\"fixed\")\n",
    "        else:\n",
    "            self.kernel_ = clone(self.kernel)\n",
    "\n",
    "        self.rng = check_random_state(self.random_state)\n",
    "\n",
    "        self.X_train_ = np.copy(X) if self.copy_X_train else X\n",
    "\n",
    "        # Encode class labels and check that it is a binary classification\n",
    "        # problem\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.y_train_ = label_encoder.fit_transform(y)\n",
    "        self.classes_ = label_encoder.classes_\n",
    "        if self.classes_.size > 2:\n",
    "            raise ValueError(\"%s supports only binary classification. \"\n",
    "                             \"y contains classes %s\"\n",
    "                             % (self.__class__.__name__, self.classes_))\n",
    "        elif self.classes_.size == 1:\n",
    "            raise ValueError(\"{0:s} requires 2 classes.\".format(\n",
    "                self.__class__.__name__))\n",
    "\n",
    "        if self.optimizer is not None and self.kernel_.n_dims > 0:\n",
    "            # Choose hyperparameters based on maximizing the log-marginal\n",
    "            # likelihood (potentially starting from several initial values)\n",
    "            def obj_func(theta, eval_gradient=True):\n",
    "                if eval_gradient:\n",
    "                    lml, grad = self.log_marginal_likelihood(\n",
    "                        theta, eval_gradient=True)\n",
    "                    return -lml, -grad\n",
    "                else:\n",
    "                    return -self.log_marginal_likelihood(theta)\n",
    "\n",
    "            # First optimize starting from theta specified in kernel\n",
    "            optima = [self._constrained_optimization(obj_func,\n",
    "                                                     self.kernel_.theta,\n",
    "                                                     self.kernel_.bounds)]\n",
    "\n",
    "            # Additional runs are performed from log-uniform chosen initial\n",
    "            # theta\n",
    "            if self.n_restarts_optimizer > 0:\n",
    "                if not np.isfinite(self.kernel_.bounds).all():\n",
    "                    raise ValueError(\n",
    "                        \"Multiple optimizer restarts (n_restarts_optimizer>0) \"\n",
    "                        \"requires that all bounds are finite.\")\n",
    "                bounds = self.kernel_.bounds\n",
    "                for iteration in range(self.n_restarts_optimizer):\n",
    "                    theta_initial = np.exp(self.rng.uniform(bounds[:, 0],\n",
    "                                                            bounds[:, 1]))\n",
    "                    optima.append(\n",
    "                        self._constrained_optimization(obj_func, theta_initial,\n",
    "                                                       bounds))\n",
    "            # Select result from run with minimal (negative) log-marginal\n",
    "            # likelihood\n",
    "            lml_values = list(map(itemgetter(1), optima))\n",
    "            self.kernel_.theta = optima[np.argmin(lml_values)][0]\n",
    "            self.log_marginal_likelihood_value_ = -np.min(lml_values)\n",
    "        else:\n",
    "            self.log_marginal_likelihood_value_ = \\\n",
    "                self.log_marginal_likelihood(self.kernel_.theta)\n",
    "\n",
    "        # Precompute quantities required for predictions which are independent\n",
    "        # of actual query points\n",
    "        K = self.kernel_(self.X_train_)\n",
    "\n",
    "        _, (self.pi_, self.W_sr_, self.L_, _, _) = \\\n",
    "            self._posterior_mode(K, return_temporaries=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Perform classification on an array of test vectors X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape = (n_samples,)\n",
    "            Predicted target values for X, values are from ``classes_``\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"X_train_\", \"y_train_\", \"pi_\", \"W_sr_\", \"L_\"])\n",
    "\n",
    "        # As discussed on Section 3.4.2 of GPML, for making hard binary\n",
    "        # decisions, it is enough to compute the MAP of the posterior and\n",
    "        # pass it through the link function\n",
    "        K_star = self.kernel_(self.X_train_, X)  # K_star =k(x_star)\n",
    "        f_star = K_star.T.dot(self.y_train_ - self.pi_)  # Algorithm 3.2,Line 4\n",
    "\n",
    "        return np.where(f_star > 0, self.classes_[1], self.classes_[0])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probability estimates for the test vector X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = (n_samples, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = (n_samples, n_classes)\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute ``classes_``.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"X_train_\", \"y_train_\", \"pi_\", \"W_sr_\", \"L_\"])\n",
    "\n",
    "        # Based on Algorithm 3.2 of GPML\n",
    "        K_star = self.kernel_(self.X_train_, X)  # K_star =k(x_star)\n",
    "        f_star = K_star.T.dot(self.y_train_ - self.pi_)  # Line 4\n",
    "        v = solve(self.L_, self.W_sr_[:, np.newaxis] * K_star)  # Line 5\n",
    "        # Line 6 (compute np.diag(v.T.dot(v)) via einsum)\n",
    "        var_f_star = self.kernel_.diag(X) - np.einsum(\"ij,ij->j\", v, v)\n",
    "\n",
    "        alpha = 1 / (2 * var_f_star)\n",
    "        gamma = LAMBDAS * f_star\n",
    "        integrals = np.sqrt(np.pi / alpha) \\\n",
    "            * erf(gamma * np.sqrt(alpha / (alpha + LAMBDAS**2))) \\\n",
    "            / (2 * np.sqrt(var_f_star * 2 * np.pi))\n",
    "        pi_star = (COEFS * integrals).sum(axis=0) + .5 * COEFS.sum()\n",
    "\n",
    "        return np.vstack((1 - pi_star, pi_star)).T,f_star,var_f_star\n",
    "\n",
    "    def log_marginal_likelihood(self, theta=None, eval_gradient=False):\n",
    "        \"\"\"Returns log-marginal likelihood of theta for training data.\n",
    "        \"\"\"\n",
    "        if theta is None:\n",
    "            if eval_gradient:\n",
    "                raise ValueError(\n",
    "                    \"Gradient can only be evaluated for theta!=None\")\n",
    "            return self.log_marginal_likelihood_value_\n",
    "\n",
    "        kernel = self.kernel_.clone_with_theta(theta)\n",
    "\n",
    "        if eval_gradient:\n",
    "            K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n",
    "        else:\n",
    "            K = kernel(self.X_train_)\n",
    "\n",
    "        # Compute log-marginal-likelihood Z and also store some temporaries\n",
    "        # which can be reused for computing Z's gradient\n",
    "        Z, (pi, W_sr, L, b, a) = \\\n",
    "            self._posterior_mode(K, return_temporaries=True)\n",
    "\n",
    "        if not eval_gradient:\n",
    "            return Z\n",
    "\n",
    "        # Compute gradient based on Algorithm 5.1 of GPML\n",
    "        d_Z = np.empty(theta.shape[0])\n",
    "        # XXX: Get rid of the np.diag() in the next line\n",
    "        R = W_sr[:, np.newaxis] * cho_solve((L, True), np.diag(W_sr))  # Line 7\n",
    "        C = solve(L, W_sr[:, np.newaxis] * K)  # Line 8\n",
    "        # Line 9: (use einsum to compute np.diag(C.T.dot(C))))\n",
    "        s_2 = -0.5 * (np.diag(K) - np.einsum('ij, ij -> j', C, C)) \\\n",
    "            * (pi * (1 - pi) * (1 - 2 * pi))  # third derivative\n",
    "\n",
    "        for j in range(d_Z.shape[0]):\n",
    "            C = K_gradient[:, :, j]   # Line 11\n",
    "            # Line 12: (R.T.ravel().dot(C.ravel()) = np.trace(R.dot(C)))\n",
    "            s_1 = .5 * a.T.dot(C).dot(a) - .5 * R.T.ravel().dot(C.ravel())\n",
    "\n",
    "            b = C.dot(self.y_train_ - pi)  # Line 13\n",
    "            s_3 = b - K.dot(R.dot(b))  # Line 14\n",
    "\n",
    "            d_Z[j] = s_1 + s_2.T.dot(s_3)  # Line 15\n",
    "\n",
    "        return Z, d_Z\n",
    "\n",
    "    def _posterior_mode(self, K, return_temporaries=False):\n",
    "        \"\"\"Mode-finding for binary Laplace GPC and fixed kernel.\n",
    "\n",
    "        This approximates the posterior of the latent function values for given\n",
    "        inputs and target observations with a Gaussian approximation and uses\n",
    "        Newton's iteration to find the mode of this approximation.\n",
    "        \"\"\"\n",
    "        # Based on Algorithm 3.1 of GPML\n",
    "\n",
    "        # If warm_start are enabled, we reuse the last solution for the\n",
    "        # posterior mode as initialization; otherwise, we initialize with 0\n",
    "        if self.warm_start and hasattr(self, \"f_cached\") \\\n",
    "           and self.f_cached.shape == self.y_train_.shape:\n",
    "            f = self.f_cached\n",
    "        else:\n",
    "            f = np.zeros_like(self.y_train_, dtype=np.float64)\n",
    "\n",
    "        # Use Newton's iteration method to find mode of Laplace approximation\n",
    "        log_marginal_likelihood = -np.inf\n",
    "        for _ in range(self.max_iter_predict):\n",
    "            # Line 4\n",
    "            pi = expit(f)\n",
    "            W = pi * (1 - pi)\n",
    "            # Line 5\n",
    "            W_sr = np.sqrt(W)\n",
    "            W_sr_K = W_sr[:, np.newaxis] * K\n",
    "            B = np.eye(W.shape[0]) + W_sr_K * W_sr\n",
    "            L = cholesky(B, lower=True)\n",
    "            # Line 6\n",
    "            b = W * f + (self.y_train_ - pi)\n",
    "            # Line 7\n",
    "            a = b - W_sr * cho_solve((L, True), W_sr_K.dot(b))\n",
    "            # Line 8\n",
    "            f = K.dot(a)\n",
    "\n",
    "            # Line 10: Compute log marginal likelihood in loop and use as\n",
    "            #          convergence criterion\n",
    "            lml = -0.5 * a.T.dot(f) \\\n",
    "                - np.log(1 + np.exp(-(self.y_train_ * 2 - 1) * f)).sum() \\\n",
    "                - np.log(np.diag(L)).sum()\n",
    "            # Check if we have converged (log marginal likelihood does\n",
    "            # not decrease)\n",
    "            # XXX: more complex convergence criterion\n",
    "            if lml - log_marginal_likelihood < 1e-10:\n",
    "                break\n",
    "            log_marginal_likelihood = lml\n",
    "\n",
    "        self.f_cached = f  # Remember solution for later warm-starts\n",
    "        if return_temporaries:\n",
    "            return log_marginal_likelihood, (pi, W_sr, L, b, a)\n",
    "        else:\n",
    "            return log_marginal_likelihood\n",
    "\n",
    "    def _constrained_optimization(self, obj_func, initial_theta, bounds):\n",
    "        if self.optimizer == \"fmin_l_bfgs_b\":\n",
    "            theta_opt, func_min, convergence_dict = \\\n",
    "                fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)\n",
    "            if convergence_dict[\"warnflag\"] != 0:\n",
    "                warnings.warn(\"fmin_l_bfgs_b terminated abnormally with the \"\n",
    "                              \" state: %s\" % convergence_dict)\n",
    "        elif callable(self.optimizer):\n",
    "            theta_opt, func_min = \\\n",
    "                self.optimizer(obj_func, initial_theta, bounds=bounds)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer %s.\" % self.optimizer)\n",
    "\n",
    "        return theta_opt, func_min\n",
    "\n",
    "\n",
    "class GaussianProcessClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, kernel=None, optimizer=\"fmin_l_bfgs_b\",\n",
    "                 n_restarts_optimizer=0, max_iter_predict=100,\n",
    "                 warm_start=False, copy_X_train=True, random_state=None,\n",
    "                 multi_class=\"one_vs_rest\", n_jobs=1):\n",
    "        self.kernel = kernel\n",
    "        self.optimizer = optimizer\n",
    "        self.n_restarts_optimizer = n_restarts_optimizer\n",
    "        self.max_iter_predict = max_iter_predict\n",
    "        self.warm_start = warm_start\n",
    "        self.copy_X_train = copy_X_train\n",
    "        self.random_state = random_state\n",
    "        self.multi_class = multi_class\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, multi_output=False)\n",
    "\n",
    "        self.base_estimator_ = _BinaryGaussianProcessClassifierLaplace(\n",
    "            self.kernel, self.optimizer, self.n_restarts_optimizer,\n",
    "            self.max_iter_predict, self.warm_start, self.copy_X_train,\n",
    "            self.random_state)\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = self.classes_.size\n",
    "        if self.n_classes_ == 1:\n",
    "            raise ValueError(\"GaussianProcessClassifier requires 2 or more \"\n",
    "                             \"distinct classes. Only class %s present.\"\n",
    "                             % self.classes_[0])\n",
    "        if self.n_classes_ > 2:\n",
    "            if self.multi_class == \"one_vs_rest\":\n",
    "                self.base_estimator_ = \\\n",
    "                    OneVsRestClassifier(self.base_estimator_,\n",
    "                                        n_jobs=self.n_jobs)\n",
    "            elif self.multi_class == \"one_vs_one\":\n",
    "                self.base_estimator_ = \\\n",
    "                    OneVsOneClassifier(self.base_estimator_,\n",
    "                                       n_jobs=self.n_jobs)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown multi-class mode %s\"\n",
    "                                 % self.multi_class)\n",
    "\n",
    "        self.base_estimator_.fit(X, y)\n",
    "\n",
    "        if self.n_classes_ > 2:\n",
    "            self.log_marginal_likelihood_value_ = np.mean(\n",
    "                [estimator.log_marginal_likelihood()\n",
    "                 for estimator in self.base_estimator_.estimators_])\n",
    "        else:\n",
    "            self.log_marginal_likelihood_value_ = \\\n",
    "                self.base_estimator_.log_marginal_likelihood()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, [\"classes_\", \"n_classes_\"])\n",
    "        X = check_array(X)\n",
    "        return self.base_estimator_.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, [\"classes_\", \"n_classes_\"])\n",
    "        if self.n_classes_ > 2 and self.multi_class == \"one_vs_one\":\n",
    "            raise ValueError(\"one_vs_one multi-class mode does not support \"\n",
    "                             \"predicting probability estimates. Use \"\n",
    "                             \"one_vs_rest mode instead.\")\n",
    "        X = check_array(X)\n",
    "        return self.base_estimator_.predict_proba(X)\n",
    "\n",
    "    @property\n",
    "    def kernel_(self):\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.base_estimator_.kernel_\n",
    "        else:\n",
    "            return CompoundKernel(\n",
    "                [estimator.kernel_\n",
    "                 for estimator in self.base_estimator_.estimators_])\n",
    "\n",
    "    def log_marginal_likelihood(self, theta=None, eval_gradient=False):\n",
    "        check_is_fitted(self, [\"classes_\", \"n_classes_\"])\n",
    "\n",
    "        if theta is None:\n",
    "            if eval_gradient:\n",
    "                raise ValueError(\n",
    "                    \"Gradient can only be evaluated for theta!=None\")\n",
    "            return self.log_marginal_likelihood_value_\n",
    "\n",
    "        theta = np.asarray(theta)\n",
    "        if self.n_classes_ == 2:\n",
    "            return self.base_estimator_.log_marginal_likelihood(\n",
    "                theta, eval_gradient)\n",
    "        else:\n",
    "            if eval_gradient:\n",
    "                raise NotImplementedError(\n",
    "                    \"Gradient of log-marginal-likelihood not implemented for \"\n",
    "                    \"multi-class GPC.\")\n",
    "            estimators = self.base_estimator_.estimators_\n",
    "            n_dims = estimators[0].kernel_.n_dims\n",
    "            if theta.shape[0] == n_dims:  # use same theta for all sub-kernels\n",
    "                return np.mean(\n",
    "                    [estimator.log_marginal_likelihood(theta)\n",
    "                     for i, estimator in enumerate(estimators)])\n",
    "            elif theta.shape[0] == n_dims * self.classes_.shape[0]:\n",
    "                # theta for compound kernel\n",
    "                return np.mean(\n",
    "                    [estimator.log_marginal_likelihood(\n",
    "                        theta[n_dims * i:n_dims * (i + 1)])\n",
    "                     for i, estimator in enumerate(estimators)])\n",
    "            else:\n",
    "                raise ValueError(\"Shape of theta must be either %d or %d. \"\n",
    "                                 \"Obtained theta with shape %d.\"\n",
    "                                 % (n_dims, n_dims * self.classes_.shape[0],\n",
    "                                    theta.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_rel = 1 # threshold for q_relative criteria\n",
    "kern = 1.0*RBF(1.0)\n",
    "auc=[]\n",
    "ffpr=[]\n",
    "ttpr=[]\n",
    "\n",
    "for j in range(10):\n",
    "    gpc = GaussianProcessClassifier(kernel=kern,random_state=0)\n",
    "\n",
    "    #dividing the total dataset into 60% training and 40% test\n",
    "    total_no = len(total_data_objects)\n",
    "    print(\"total objects=\",total_no)\n",
    "    train_no=round(0.6*total_no)\n",
    "    print(\"train objects=\",train_no)\n",
    "    test_no=round(0.4*total_no)\n",
    "    print(\"test objects=\",test_no)\n",
    "\n",
    "    X_test=[]\n",
    "    Y_test=[]\n",
    "    train_objects= []\n",
    "\n",
    "    #for taking two normal and one abnormal as new train to GPClassifier.\n",
    "    feature_train = []\n",
    "    label_train = []\n",
    "    ab=0\n",
    "    n=0\n",
    "    \n",
    "    np.random.shuffle(total_data_objects)\n",
    "    for i in range(train_no):\n",
    "        if total_data_objects[i].label == -1 and n < 2:\n",
    "            feature_train.append(total_data_objects[i].weights_data)\n",
    "            label_train.append(total_data_objects[i].label)\n",
    "            n+=1\n",
    "        elif total_data_objects[i].label == 1 and ab < 1:\n",
    "            feature_train.append(total_data_objects[i].weights_data)\n",
    "            label_train.append(total_data_objects[i].label)\n",
    "            ab+=1\n",
    "        else :\n",
    "            train_objects.append(total_data_objects[i])\n",
    "\n",
    "    for i in range(train_no,total_no):\n",
    "        X_test.append(total_data_objects[i].weights_data)\n",
    "        Y_test.append(total_data_objects[i].label)\n",
    "\n",
    "    feature_train = np.asarray(feature_train)\n",
    "    label_train = np.asarray(label_train).reshape(-1,1)\n",
    "    X_test=np.asarray(X_test)\n",
    "    Y_test=np.asarray(Y_test)\n",
    "\n",
    "    print(\"initial feature length\",len(feature_train))\n",
    "    for val in range(10): \n",
    "        q=0\n",
    "        new_objects = []\n",
    "        # training the objects\n",
    "        for i in range(len(train_objects)):\n",
    "            gpc.fit(feature_train,label_train)\n",
    "            prob,mu_s,cov_s = gpc.predict_proba(train_objects[i].weights_data.reshape(1,-1))\n",
    "            q_rel = min(2*abs(mu_s),2/abs(np.sqrt(cov_s)))  # calculating q rel criteria\n",
    "            \n",
    "            #print(\"clip \",i,\" q_rel vale \",q_rel)\n",
    "            if q_rel < t_rel:    # if q_rel is less than given threshold then give the clip to domain expert\n",
    "                q+=1\n",
    "                '''\n",
    "                print(\"enter label for clip no: \",train_objects[i].clip_no,\"with start time \",train_objects[i].stime,\" end time \",train_objects[i].etime)\n",
    "                ll = int(input())   # taking input from user(domain_expert)\n",
    "                '''\n",
    "\n",
    "                ll=train_objects[i].label\n",
    "                feature_train = np.vstack([feature_train,train_objects[i].weights_data]) #adding newly labeled sample to training features\n",
    "                label_train = np.append(label_train,ll)  # adding new label to training labels\n",
    "            else:\n",
    "                new_objects.append(train_objects[i])\n",
    "\n",
    "        gpc.fit(feature_train,label_train)\n",
    "        pred_labels=gpc.predict(X_test)\n",
    "        train_objects.clear()\n",
    "        print(\"total no of queries \",q)\n",
    "        train_objects = new_objects.copy()\n",
    "        print(\"feature length\",len(feature_train))\n",
    "        print(\"train length\",len(train_objects))\n",
    "\n",
    "        #performance\n",
    "        print(\"accuracy =\",accuracy_score(pred_labels,Y_test)*100)\n",
    "        print(\"confusion matrix\",confusion_matrix(pred_labels,Y_test))\n",
    "\n",
    "    auc.append(roc_auc_score(Y_test,pred_labels))\n",
    "    print('AUC:',auc[j])\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,pred_labels)\n",
    "    ffpr.append(fpr)\n",
    "    ttpr.append(tpr)\n",
    "    plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_auc = 0\n",
    "sum_tpr = 0\n",
    "sum_fpr = 0\n",
    "ffpr = np.asarray(ffpr)\n",
    "print(ffpr)\n",
    "ttpr= np.asarray(ttpr)\n",
    "print(ttpr)\n",
    "\"\"\"\n",
    "for i in range(len(auc)):\n",
    "    sum_auc += auc[i]\n",
    "    sum_tpr += ttpr[i]\n",
    "    sum_fpr += ffpr[i]\n",
    "\"\"\"\n",
    "print(\"avg auc score\",np.mean(auc))\n",
    "#plot_roc_curve(np.mean(ffpr),np.mean(ttpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
